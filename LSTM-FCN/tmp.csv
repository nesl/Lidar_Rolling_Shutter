Num datasets :  128

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 1, 1600)      0                                            
__________________________________________________________________________________________________
permute_1 (Permute)             (None, 1600, 1)      0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 1600, 128)    1152        permute_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 1600, 128)    512         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 1600, 128)    0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 1600, 256)    164096      activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 1600, 256)    1024        conv1d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 1600, 256)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 1600, 128)    98432       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 1600, 128)    512         conv1d_3[0][0]                   
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 64)           426240      input_1[0][0]                    
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 1600, 128)    0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 64)           0           lstm_1[0][0]                     
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 192)          0           dropout_1[0][0]                  
                                                                 global_average_pooling1d_1[0][0] 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            193         concatenate_1[0][0]              
==================================================================================================
Total params: 692,161
Trainable params: 691,137
Non-trainable params: 1,024
__________________________________________________________________________________________________
******************** Training model for dataset  ********************
Finished loading train dataset..
Weights loaded from  ./weights/_weights.h5

Evaluating : 

128/574 [=====>........................] - ETA: 4s
256/574 [============>.................] - ETA: 2s
384/574 [===================>..........] - ETA: 1s
512/574 [=========================>....] - ETA: 0s
574/574 [==============================] - 5s 9ms/step

Final loss :  4291.736277085148
100 , 176.59787
100 , 150.73798
100 , 165.85495
100 , 168.26776
100 , 133.01434
100 , 189.22983
100 , 141.31625
100 , 185.50726
100 , 184.50894
100 , 117.94395
100 , 163.55351
100 , 164.94405
100 , 173.13248
100 , 189.98926
110 , 226.19052
110 , 181.68259
110 , 176.64679
110 , 206.57597
110 , 167.42146
110 , 219.86517
110 , 170.56566
110 , 200.90793
110 , 176.31409
110 , 151.17838
110 , 223.04471
110 , 207.97147
110 , 177.4716
110 , 210.53624
120 , 203.9892
120 , 197.11844
120 , 200.17947
120 , 204.58722
120 , 177.05124
120 , 224.11526
120 , 192.91316
120 , 201.08505
120 , 237.64438
120 , 185.87332
120 , 245.43927
120 , 195.431
120 , 173.76718
120 , 203.61588
130 , 222.15633
130 , 229.43857
130 , 251.0133
130 , 237.34662
130 , 207.30151
130 , 275.7416
130 , 259.60797
130 , 254.84834
130 , 229.22601
130 , 232.80719
130 , 287.64322
130 , 237.0545
130 , 262.397
130 , 263.21228
140 , 221.90077
140 , 264.40887
140 , 234.7768
140 , 213.55733
140 , 194.8498
140 , 260.4012
140 , 243.1748
140 , 269.73392
140 , 226.08972
140 , 270.75992
140 , 268.85745
140 , 207.16937
140 , 230.37573
140 , 257.43286
150 , 285.27078
150 , 204.8136
150 , 266.88644
150 , 258.85416
150 , 226.62408
150 , 265.44443
150 , 220.11192
150 , 238.96198
150 , 285.26566
150 , 198.48471
150 , 248.87534
150 , 254.24516
150 , 218.88821
150 , 203.69849
160 , 229.48096
160 , 229.28363
160 , 228.89532
160 , 250.2692
160 , 211.70782
160 , 230.40305
160 , 262.76767
160 , 259.33075
160 , 285.4878
160 , 218.60843
160 , 257.6959
160 , 238.81442
160 , 238.51715
160 , 272.3507
170 , 250.55524
170 , 266.9551
170 , 236.67024
170 , 270.4469
170 , 214.0567
170 , 250.63779
170 , 211.62535
170 , 234.503
170 , 213.52618
170 , 209.32053
170 , 259.7161
170 , 237.71983
170 , 275.06705
170 , 291.86002
180 , 289.21652
180 , 244.97647
180 , 230.38809
180 , 296.75613
180 , 217.8807
180 , 234.89218
180 , 238.37036
180 , 251.62737
180 , 249.11081
180 , 186.0827
180 , 218.68036
180 , 220.35123
180 , 235.08504
180 , 212.70563
190 , 305.70944
190 , 235.34058
190 , 302.5074
190 , 245.83347
190 , 229.99734
190 , 262.548
190 , 238.82294
190 , 281.96252
190 , 265.999
190 , 251.18863
190 , 256.8508
190 , 249.84991
190 , 215.92007
190 , 292.0596
200 , 239.77654
200 , 239.64159
200 , 258.0948
200 , 303.24527
200 , 235.67365
200 , 265.16196
200 , 264.6282
200 , 315.18814
200 , 235.2596
200 , 217.72894
200 , 249.86476
200 , 261.0243
200 , 238.58469
200 , 279.04385
210 , 271.32693
210 , 249.63571
210 , 272.04758
210 , 245.52759
210 , 199.8248
210 , 273.5051
210 , 261.54837
210 , 245.48892
210 , 265.69754
210 , 267.309
210 , 268.2432
210 , 230.5356
210 , 280.29272
210 , 278.23233
220 , 274.73608
220 , 234.84323
220 , 277.61337
220 , 276.40036
220 , 262.84058
220 , 300.67612
220 , 260.9711
220 , 245.54213
220 , 277.94962
220 , 219.38765
220 , 275.77075
220 , 266.85684
220 , 280.94852
220 , 258.64575
230 , 256.7893
230 , 278.86423
230 , 248.27396
230 , 258.07343
230 , 258.6001
230 , 262.91537
230 , 296.52463
230 , 303.5544
230 , 272.81595
230 , 227.68857
230 , 268.27676
230 , 267.34924
230 , 278.9681
230 , 295.47733
240 , 239.67445
240 , 228.07317
240 , 266.53928
240 , 228.3097
240 , 232.30757
240 , 241.12228
240 , 223.21808
240 , 299.5845
240 , 334.05197
240 , 276.90067
240 , 246.55504
240 , 250.15738
240 , 240.28006
240 , 250.5975
250 , 269.82404
250 , 190.74277
250 , 278.16446
250 , 252.396
250 , 259.59607
250 , 308.4378
250 , 257.0015
250 , 272.5289
250 , 270.04196
250 , 244.7214
250 , 294.32712
250 , 280.75806
250 , 305.83066
250 , 281.91284
260 , 324.99783
260 , 284.16812
260 , 349.82397
260 , 286.23154
260 , 347.8884
260 , 316.83676
260 , 304.77826
260 , 358.66153
260 , 284.96353
260 , 274.83023
260 , 341.93246
260 , 288.26218
260 , 328.17337
260 , 310.077
270 , 301.85223
270 , 251.69751
270 , 299.75156
270 , 264.24417
270 , 267.81085
270 , 307.59366
270 , 229.5736
270 , 286.47598
270 , 297.3159
270 , 230.5726
270 , 287.05603
270 , 325.33435
270 , 271.63583
270 , 286.30457
280 , 288.76315
280 , 331.95712
280 , 288.78226
280 , 328.55716
280 , 310.87457
280 , 314.9695
280 , 274.64746
280 , 306.21808
280 , 297.92215
280 , 289.8027
280 , 312.0801
280 , 302.98895
280 , 331.06125
280 , 280.59912
290 , 296.2147
290 , 319.04834
290 , 345.29233
290 , 287.88992
290 , 250.98643
290 , 372.3577
290 , 293.75232
290 , 293.05075
290 , 333.6567
290 , 283.05203
290 , 300.47757
290 , 242.99855
290 , 306.35165
290 , 296.52655
300 , 354.45065
300 , 317.24545
300 , 293.267
300 , 333.90973
300 , 323.21497
300 , 362.09854
300 , 287.44583
300 , 338.80823
300 , 301.33694
300 , 282.5478
300 , 327.36008
300 , 317.1487
300 , 312.08524
300 , 316.5347
310 , 318.11917
310 , 288.84714
310 , 225.79489
310 , 306.26437
310 , 270.3661
310 , 310.50455
310 , 295.425
310 , 297.4832
310 , 292.59683
310 , 290.24078
310 , 236.7833
310 , 295.05887
310 , 334.11014
310 , 312.16455
320 , 303.26016
320 , 306.65244
320 , 324.03094
320 , 316.6647
320 , 316.1937
320 , 313.94626
320 , 272.7808
320 , 297.15054
320 , 292.40244
320 , 258.8997
320 , 337.1587
320 , 311.34225
320 , 309.55762
320 , 301.43408
330 , 313.03378
330 , 293.55502
330 , 328.21927
330 , 328.77454
330 , 262.9036
330 , 301.79477
330 , 247.35431
330 , 339.56076
330 , 340.2624
330 , 297.322
330 , 307.70593
330 , 302.77023
330 , 265.79037
330 , 269.03476
340 , 359.7402
340 , 318.80142
340 , 339.1307
340 , 336.35608
340 , 299.75223
340 , 315.5939
340 , 316.89853
340 , 358.43802
340 , 329.8478
340 , 313.60373
340 , 354.22015
340 , 300.15726
340 , 342.88605
340 , 350.47183
350 , 295.2702
350 , 290.55194
350 , 257.7695
350 , 305.89984
350 , 287.18552
350 , 332.51077
350 , 288.5881
350 , 306.9738
350 , 314.033
350 , 341.13492
350 , 307.96097
350 , 311.69565
350 , 286.17712
350 , 342.71347
360 , 319.42618
360 , 344.24393
360 , 362.79025
360 , 345.51334
360 , 337.6665
360 , 359.83023
360 , 353.15994
360 , 338.42813
360 , 342.1215
360 , 340.71478
360 , 340.03305
360 , 374.37238
360 , 336.08078
360 , 320.59262
370 , 377.16232
370 , 306.00803
370 , 347.297
370 , 366.7235
370 , 352.51926
370 , 327.6993
370 , 284.1209
370 , 333.1985
370 , 309.7963
370 , 324.4103
370 , 360.3008
370 , 286.26898
370 , 369.2813
370 , 380.69406
380 , 340.7407
380 , 413.48074
380 , 332.13733
380 , 365.77896
380 , 330.25418
380 , 412.4326
380 , 343.04572
380 , 391.15323
380 , 358.5317
380 , 350.80795
380 , 406.35495
380 , 339.7908
380 , 341.99524
380 , 354.61398
390 , 361.61386
390 , 388.16852
390 , 401.3633
390 , 372.14133
390 , 312.66968
390 , 365.75076
390 , 416.56592
390 , 368.28445
390 , 366.66788
390 , 345.66815
390 , 386.81625
390 , 352.36432
390 , 382.2564
390 , 391.55994
400 , 375.4786
400 , 306.3342
400 , 371.27115
400 , 364.47485
400 , 320.36618
400 , 354.11783
400 , 360.7009
400 , 385.90793
400 , 348.9047
400 , 310.21475
400 , 312.43082
400 , 345.41928
400 , 321.60645
400 , 353.65295
410 , 398.57623
410 , 360.8505
410 , 450.75247
410 , 385.18362
410 , 356.8654
410 , 343.1019
410 , 476.1356
410 , 424.31885
410 , 380.33206
410 , 353.1068
410 , 382.45834
410 , 388.59933
410 , 388.12375
410 , 340.0278
420 , 317.47812
420 , 326.9407
420 , 405.059
420 , 337.3195
420 , 337.46548
420 , 380.7781
420 , 300.084
420 , 329.39456
420 , 356.50412
420 , 326.56143
420 , 332.66238
420 , 351.2684
420 , 316.00043
420 , 380.28537
430 , 395.4515
430 , 364.0375
430 , 401.27216
430 , 385.2321
430 , 296.54135
430 , 376.8398
430 , 408.9142
430 , 413.00485
430 , 409.94266
430 , 359.108
430 , 425.37473
430 , 348.0742
430 , 385.12912
430 , 376.65317
440 , 363.3314
440 , 448.42343
440 , 345.65536
440 , 365.99496
440 , 315.217
440 , 394.5268
440 , 399.84323
440 , 370.39542
440 , 371.15106
440 , 378.80197
440 , 485.6445
440 , 401.47263
440 , 371.06952
440 , 397.82443
450 , 422.67593
450 , 348.9746
450 , 372.54282
450 , 378.3954
450 , 416.21072
450 , 406.23447
450 , 331.2077
450 , 366.58328
450 , 411.36942
450 , 343.92844
450 , 363.70218
450 , 391.78177
450 , 373.92325
450 , 378.55634
460 , 385.3417
460 , 398.2749
460 , 360.05362
460 , 393.3363
460 , 396.24408
460 , 441.4639
460 , 396.32272
460 , 405.03384
460 , 379.7271
460 , 363.947
460 , 409.44626
460 , 370.80545
460 , 426.29605
460 , 430.36783
470 , 381.57642
470 , 326.43106
470 , 377.38293
470 , 366.41675
470 , 338.12363
470 , 384.6735
470 , 373.4125
470 , 376.7723
470 , 404.83725
470 , 372.69046
470 , 408.0822
470 , 383.52084
470 , 345.60306
470 , 364.61322
480 , 400.7518
480 , 418.5317
480 , 404.38773
480 , 433.56512
480 , 396.90265
480 , 435.3198
480 , 375.64
480 , 414.5584
480 , 371.72455
480 , 373.67596
480 , 432.21255
480 , 448.70462
480 , 360.77533
480 , 367.08276
490 , 419.29303
490 , 405.65826
490 , 397.28476
490 , 425.9544
490 , 350.50272
490 , 438.30566
490 , 350.18555
490 , 393.48746
490 , 395.96817
490 , 324.70883
490 , 429.13266
490 , 393.34042
490 , 408.96664
490 , 374.73447
500 , 416.32938
500 , 367.81235
500 , 428.2837
500 , 380.6782
500 , 379.03
500 , 387.5306
500 , 428.00217
500 , 411.11008
500 , 457.30917
500 , 374.116
500 , 444.70154
500 , 405.25653
500 , 402.46634
500 , 379.63174
